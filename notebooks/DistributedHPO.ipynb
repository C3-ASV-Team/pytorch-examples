{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Hyper-parameter Optimization on Cori notebook\n",
    "\n",
    "In this notebook we will develop a Cori example for distributed HPO with distributed training.\n",
    "\n",
    "We will demonstrate the following:\n",
    "1. Show how to invoke the command-line training script from the notebook\n",
    "2. Launch the single job on the batch system using the SlurmJob API.\n",
    "3. Demonstrate distributed HPO with multi-node training using SlurmJob.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/s/sfarrell/WorkAreas/jupyter-dl/pytorch-examples\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.slurm_helpers import SlurmJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "Let's start by editing our environment directly.\n",
    "I might also prefer to do this in the kernel json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] = '/usr/common/software/pytorch/v0.4.1/bin:' + os.environ['PATH']\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/common/software/pytorch/v0.4.1/lib:' + os.environ['LD_LIBRARY_PATH']\n",
    "os.environ['MPICH_MAX_THREAD_SAFETY'] = 'multiple'\n",
    "os.environ['KMP_AFFINITY'] = 'granularity=fine,compact,1,0'\n",
    "os.environ['KMP_BLOCKTIME'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_dir = os.path.expandvars('$SCRATCH/pytorch-examples/mnist-hpo')\n",
    "os.makedirs(hpo_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(output_dir, conv_sizes, dense_sizes,\n",
    "                 learning_rate=0.001, optimizer='Adam',\n",
    "                 batch_size=64, n_epochs=1):\n",
    "    data_config = dict(name='mnist', data_path='$SCRATCH/pytorch-mnist/data')\n",
    "    experiment_config = dict(name='basic', output_dir=output_dir)\n",
    "    model_config = dict(\n",
    "        model_type='cnn_classifier',\n",
    "        input_shape=[1, 28, 28], n_classes=10,\n",
    "        conv_sizes=conv_sizes, dense_sizes=dense_sizes,\n",
    "        optimizer=optimizer, learning_rate=learning_rate\n",
    "    )\n",
    "    train_config = dict(batch_size=batch_size, n_epochs=n_epochs)\n",
    "    return dict(data_config=data_config, experiment_config=experiment_config,\n",
    "                model_config=model_config, train_config=train_config)\n",
    "\n",
    "def write_config(config, file):\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "    with open(file, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "def get_val_acc(config):\n",
    "    output_dir = os.path.expandvars(config['experiment_config']['output_dir'])\n",
    "    summaries = np.load(os.path.join(output_dir, 'summaries.npz'))\n",
    "    return summaries['valid_acc'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.expandvars('$SCRATCH/pytorch-examples/mnist-jupyter')\n",
    "config = build_config(output_dir=output_dir,\n",
    "                      conv_sizes=[8, 16, 32], dense_sizes=[])\n",
    "\n",
    "# Serialize the configuration to a temporary file\n",
    "config_file = os.path.join(output_dir, 'config.yaml')\n",
    "write_config(config, config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-16 17:18:52,485 INFO Initializing\n",
      "2018-10-16 17:18:52,488 INFO Configuration: {'data_config': {'data_path': '$SCRATCH/pytorch-mnist/data', 'name': 'mnist'}, 'experiment_config': {'name': 'basic', 'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-jupyter'}, 'model_config': {'conv_sizes': [8, 16, 32], 'dense_sizes': [], 'input_shape': [1, 28, 28], 'learning_rate': 0.001, 'model_type': 'cnn_classifier', 'n_classes': 10, 'optimizer': 'Adam'}, 'train_config': {'batch_size': 64, 'n_epochs': 1}}\n",
      "2018-10-16 17:18:52,664 INFO Loaded 60000 training samples\n",
      "2018-10-16 17:18:52,665 INFO Loaded 10000 validation samples\n",
      "2018-10-16 17:18:52,684 INFO Model: \n",
      "CNNClassifier(\n",
      "  (conv_net): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense_net): Sequential(\n",
      "    (0): Linear(in_features=288, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameters: 8778\n",
      "2018-10-16 17:18:52,684 INFO Epoch 0\n",
      "2018-10-16 17:19:15,135 INFO   Training loss: 0.287\n",
      "2018-10-16 17:19:17,263 INFO   Validation loss: 0.098 acc: 0.968\n",
      "2018-10-16 17:19:17,265 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-examples/mnist-jupyter/summaries.npz\n",
      "2018-10-16 17:19:17,276 INFO Finished training\n",
      "2018-10-16 17:19:17,277 INFO Train samples 60000 time 22.4514s rate 2672.44 samples/s\n",
      "2018-10-16 17:19:17,277 INFO Valid samples 10000 time 2.12715 s rate 4701.12 samples/s\n",
      "2018-10-16 17:19:17,277 INFO All done!\n"
     ]
    }
   ],
   "source": [
    "!python ./main.py $config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy: 0.9683\n"
     ]
    }
   ],
   "source": [
    "print('Validation set accuracy:', get_val_acc(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training on batch system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job configuration\n",
    "job_config = dict(\n",
    "    node_type='haswell',\n",
    "    n_nodes=1,\n",
    "    qos='interactive',\n",
    "    time=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "salloc: Pending job allocation 15801422\n",
      "salloc: job 15801422 queued and waiting for resources\n",
      "salloc: job 15801422 has been allocated resources\n",
      "salloc: Granted job allocation 15801422\n",
      "salloc: Waiting for resource configuration\n",
      "salloc: Nodes nid00082 are ready for job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = SlurmJob(**job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, err = job.submit_task('python ./main.py %s' % config_file).communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "2018-10-16 17:24:54,186 INFO Initializing\n",
      "2018-10-16 17:24:54,224 INFO Configuration: {'data_config': {'data_path': '$SCRATCH/pytorch-mnist/data', 'name': 'mnist'}, 'experiment_config': {'name': 'basic', 'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-jupyter'}, 'model_config': {'conv_sizes': [8, 16, 32], 'dense_sizes': [], 'input_shape': [1, 28, 28], 'learning_rate': 0.001, 'model_type': 'cnn_classifier', 'n_classes': 10, 'optimizer': 'Adam'}, 'train_config': {'batch_size': 64, 'n_epochs': 1}}\n",
      "2018-10-16 17:24:54,955 INFO Loaded 60000 training samples\n",
      "2018-10-16 17:24:54,955 INFO Loaded 10000 validation samples\n",
      "2018-10-16 17:24:55,007 INFO Model: \n",
      "CNNClassifier(\n",
      "  (conv_net): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense_net): Sequential(\n",
      "    (0): Linear(in_features=288, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameters: 8778\n",
      "2018-10-16 17:24:55,007 INFO Epoch 0\n",
      "2018-10-16 17:25:13,935 INFO   Training loss: 0.285\n",
      "2018-10-16 17:25:15,817 INFO   Validation loss: 0.113 acc: 0.964\n",
      "2018-10-16 17:25:15,823 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-examples/mnist-jupyter/summaries.npz\n",
      "2018-10-16 17:25:15,855 INFO Finished training\n",
      "2018-10-16 17:25:15,855 INFO Train samples 60000 time 18.9272s rate 3170.03 samples/s\n",
      "2018-10-16 17:25:15,856 INFO Valid samples 10000 time 1.88238 s rate 5312.44 samples/s\n",
      "2018-10-16 17:25:15,856 INFO All done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our python logging currently goes to stderr\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy: 0.9642\n"
     ]
    }
   ],
   "source": [
    "print('Validation set accuracy:', get_val_acc(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the allocation\n",
    "del job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define HP sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hpo_trials = 32\n",
    "\n",
    "# Hyper-parameters for model config\n",
    "c1 = np.random.choice([4, 8, 16], size=n_hpo_trials)\n",
    "c2 = np.random.choice([4, 8, 16], size=n_hpo_trials)\n",
    "c3 = np.random.choice([8, 16, 32], size=n_hpo_trials)\n",
    "lr = np.random.choice([0.0001, 0.001, 0.01], size=n_hpo_trials)\n",
    "conv_sizes = np.stack([c1, c2, c3], axis=1)\n",
    "\n",
    "# Training config\n",
    "batch_size = 64\n",
    "n_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the configurations for the HPO tasks\n",
    "configs = [\n",
    "    build_config(output_dir=os.path.join(hpo_dir, 'hp_%i' % i),\n",
    "                 conv_sizes=conv_sizes[i], dense_sizes=[], learning_rate=lr[i],\n",
    "                 batch_size=batch_size, n_epochs=n_epochs)\n",
    "    for i in range(n_hpo_trials)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch HP tasks to batch system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job configuration\n",
    "job_config = dict(\n",
    "    node_type='haswell',\n",
    "    n_nodes=16,\n",
    "    qos='interactive',\n",
    "    time='2:00:00',\n",
    ")\n",
    "\n",
    "# Fix thread settings for remote job\n",
    "os.environ['OMP_NUM_THREADS'] = '32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "salloc: Pending job allocation 15801963\n",
      "salloc: job 15801963 queued and waiting for resources\n",
      "salloc: job 15801963 has been allocated resources\n",
      "salloc: Granted job allocation 15801963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the job\n",
    "job = SlurmJob(**job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-node training configuration\n",
    "n_nodes_per_task = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, config in enumerate(configs):    \n",
    "    # Write the configuration to file\n",
    "    output_dir = config['experiment_config']['output_dir']\n",
    "    config_file = os.path.join(output_dir, 'config.yaml')\n",
    "    write_config(config, config_file)\n",
    "    # Submit the task\n",
    "    results.append(job.submit_task('python ./main.py -d %s' % config_file, n_nodes=n_nodes_per_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid = job.jobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "15801963     allocation interacti+    dasrepo       1024    RUNNING      0:0 \n",
      "15801963.ex+     extern               dasrepo       1024    RUNNING      0:0 \n",
      "15801963.0       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.1       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.2       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.3       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.4       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.5       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.6       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.7       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.8       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.9       python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.10      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.11      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.12      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.13      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.14      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.15      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.16      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.17      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.18      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.19      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.20      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.21      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.22      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.23      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.24      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.25      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.26      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.27      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.28      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.29      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.30      python               dasrepo          2  COMPLETED      0:0 \n",
      "15801963.31      python               dasrepo          2  COMPLETED      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j $jobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait and gather all the results\n",
    "outputs = [r.communicate() for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "srun: Job 15801963 step creation temporarily disabled, retrying\n",
      "srun: Step created for job 15801963\n",
      "2018-10-16 17:48:57,071 INFO Initializing\n",
      "2018-10-16 17:49:00,587 INFO Initializing\n",
      "2018-10-16 17:49:00,632 INFO MPI rank 0\n",
      "2018-10-16 17:49:00,631 INFO MPI rank 1\n",
      "2018-10-16 17:49:00,655 INFO Configuration: {'data_config': {'data_path': '$SCRATCH/pytorch-mnist/data', 'name': 'mnist'}, 'experiment_config': {'name': 'basic', 'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_0'}, 'model_config': {'conv_sizes': array([ 8,  8, 32]), 'dense_sizes': [], 'input_shape': [1, 28, 28], 'learning_rate': 0.01, 'model_type': 'cnn_classifier', 'n_classes': 10, 'optimizer': 'Adam'}, 'train_config': {'batch_size': 64, 'n_epochs': 4}}\n",
      "2018-10-16 17:49:01,081 INFO Loaded 60000 training samples\n",
      "2018-10-16 17:49:01,081 INFO Loaded 10000 validation samples\n",
      "2018-10-16 17:49:02,401 INFO Loaded 60000 training samples\n",
      "2018-10-16 17:49:02,402 INFO Loaded 10000 validation samples\n",
      "2018-10-16 17:49:02,441 INFO Epoch 0\n",
      "2018-10-16 17:49:02,442 INFO Model: \n",
      "DistributedDataParallelCPU(\n",
      "  (module): CNNClassifier(\n",
      "    (conv_net): Sequential(\n",
      "      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): ReLU()\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (dense_net): Sequential(\n",
      "      (0): Linear(in_features=288, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Parameters: 5890\n",
      "2018-10-16 17:49:02,442 INFO Epoch 0\n",
      "2018-10-16 17:49:11,636 INFO   Training loss: 0.176\n",
      "2018-10-16 17:49:11,636 INFO   Training loss: 0.163\n",
      "2018-10-16 17:49:13,381 INFO   Validation loss: 0.064 acc: 0.979\n",
      "2018-10-16 17:49:13,381 INFO Epoch 1\n",
      "2018-10-16 17:49:13,384 INFO   Validation loss: 0.064 acc: 0.979\n",
      "2018-10-16 17:49:13,388 INFO Epoch 1\n",
      "2018-10-16 17:49:22,544 INFO   Training loss: 0.073\n",
      "2018-10-16 17:49:22,543 INFO   Training loss: 0.065\n",
      "2018-10-16 17:49:24,276 INFO   Validation loss: 0.064 acc: 0.980\n",
      "2018-10-16 17:49:24,276 INFO Epoch 2\n",
      "2018-10-16 17:49:24,280 INFO   Validation loss: 0.064 acc: 0.980\n",
      "2018-10-16 17:49:24,283 INFO Epoch 2\n",
      "2018-10-16 17:49:33,155 INFO   Training loss: 0.050\n",
      "2018-10-16 17:49:33,156 INFO   Training loss: 0.057\n",
      "2018-10-16 17:49:34,885 INFO   Validation loss: 0.073 acc: 0.978\n",
      "2018-10-16 17:49:34,885 INFO Epoch 3\n",
      "2018-10-16 17:49:34,890 INFO   Validation loss: 0.073 acc: 0.978\n",
      "2018-10-16 17:49:34,893 INFO Epoch 3\n",
      "2018-10-16 17:49:43,799 INFO   Training loss: 0.052\n",
      "2018-10-16 17:49:43,798 INFO   Training loss: 0.048\n",
      "2018-10-16 17:49:45,523 INFO   Validation loss: 0.052 acc: 0.985\n",
      "2018-10-16 17:49:45,522 INFO   Validation loss: 0.052 acc: 0.985\n",
      "2018-10-16 17:49:45,522 INFO Finished training\n",
      "2018-10-16 17:49:45,522 INFO Train samples 30000 time 9.03703s rate 3319.68 samples/s\n",
      "2018-10-16 17:49:45,523 INFO Valid samples 10000 time 1.73263 s rate 5771.58 samples/s\n",
      "2018-10-16 17:49:45,523 INFO All done!\n",
      "2018-10-16 17:49:45,526 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_0/summaries.npz\n",
      "2018-10-16 17:49:45,566 INFO Finished training\n",
      "2018-10-16 17:49:45,567 INFO Train samples 30000 time 9.03195s rate 3321.54 samples/s\n",
      "2018-10-16 17:49:45,567 INFO Valid samples 10000 time 1.73501 s rate 5763.67 samples/s\n",
      "2018-10-16 17:49:45,567 INFO All done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the full output from one job\n",
    "print(outputs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracies: [0.9852 0.9798 0.987  0.9485 0.94   0.9708 0.9811 0.9371 0.9849 0.9843\n",
      " 0.9708 0.9832 0.981  0.9674 0.937  0.9763 0.9352 0.9832 0.9706 0.8802\n",
      " 0.9826 0.9283 0.9132 0.9181 0.9638 0.9495 0.9855 0.9851 0.9248 0.9848\n",
      " 0.9706 0.9571]\n"
     ]
    }
   ],
   "source": [
    "# Gather the validation set accuracies\n",
    "val_accs = np.array([get_val_acc(config) for config in configs])\n",
    "print('Validation set accuracies:', val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_config': {'name': 'mnist', 'data_path': '$SCRATCH/pytorch-mnist/data'},\n",
       " 'experiment_config': {'name': 'basic',\n",
       "  'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_2'},\n",
       " 'model_config': {'model_type': 'cnn_classifier',\n",
       "  'input_shape': [1, 28, 28],\n",
       "  'n_classes': 10,\n",
       "  'conv_sizes': array([16, 16, 16]),\n",
       "  'dense_sizes': [],\n",
       "  'optimizer': 'Adam',\n",
       "  'learning_rate': 0.001},\n",
       " 'train_config': {'batch_size': 64, 'n_epochs': 4}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model configuration\n",
    "configs[val_accs.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_config': {'name': 'mnist', 'data_path': '$SCRATCH/pytorch-mnist/data'},\n",
       " 'experiment_config': {'name': 'basic',\n",
       "  'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_19'},\n",
       " 'model_config': {'model_type': 'cnn_classifier',\n",
       "  'input_shape': [1, 28, 28],\n",
       "  'n_classes': 10,\n",
       "  'conv_sizes': array([4, 4, 8]),\n",
       "  'dense_sizes': [],\n",
       "  'optimizer': 'Adam',\n",
       "  'learning_rate': 0.0001},\n",
       " 'train_config': {'batch_size': 64, 'n_epochs': 4}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Worst model configuration\n",
    "configs[val_accs.argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-v0.4.1",
   "language": "python",
   "name": "pytorch-v0.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
