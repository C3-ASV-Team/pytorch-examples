{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Hyper-parameter Optimization on Cori notebook\n",
    "\n",
    "In this notebook we will develop a Cori example for HPO.\n",
    "\n",
    "Let's develop the notebook in stages:\n",
    "1. Make it so we can invoke python and run training of a single job from the notebook on the login node.\n",
    "2. Launch the single job on the batch system using the SlurmJob API.\n",
    "3. Develop HPO logic\n",
    "4. Demonstrate distributed HPO with multi-node training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.slurm_helpers import SlurmJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "Let's start by editing our environment directly.\n",
    "I might also prefer to do this in the kernel json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] = '/usr/common/software/pytorch/v0.4.1/bin:' + os.environ['PATH']\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/common/software/pytorch/v0.4.1/lib:' + os.environ['LD_LIBRARY_PATH']\n",
    "os.environ['MPICH_MAX_THREAD_SAFETY'] = 'multiple'\n",
    "os.environ['KMP_AFFINITY'] = 'granularity=fine,compact,1,0'\n",
    "os.environ['KMP_BLOCKTIME'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/s/sfarrell/WorkAreas/dl_science_benchmarks/pytorch-examples\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(output_dir, conv_sizes, dense_sizes,\n",
    "                 learning_rate=0.001, optimizer='Adam',\n",
    "                 batch_size=64, n_epochs=1):\n",
    "    data_config = dict(name='mnist', data_path='$SCRATCH/pytorch-mnist/data')\n",
    "    experiment_config = dict(name='basic', output_dir=output_dir)\n",
    "    model_config = dict(\n",
    "        model_type='cnn_classifier',\n",
    "        input_shape=[1, 28, 28], n_classes=10,\n",
    "        conv_sizes=conv_sizes, dense_sizes=dense_sizes,\n",
    "        optimizer=optimizer, learning_rate=learning_rate\n",
    "    )\n",
    "    train_config = dict(batch_size=batch_size, n_epochs=n_epochs)\n",
    "    return dict(data_config=data_config, experiment_config=experiment_config,\n",
    "                model_config=model_config, train_config=train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_dir = os.path.expandvars('$SCRATCH/pytorch-examples/mnist-hpo')\n",
    "os.makedirs(hpo_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = build_config(output_dir=os.path.join(hpo_dir, 'output'),\n",
    "                      conv_sizes=[8, 16, 32], dense_sizes=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the configuration to a temporary file\n",
    "config_file = os.path.join(hpo_dir, 'test.yaml')\n",
    "with open(config_file, 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-15 13:28:19,639 INFO Initializing\n",
      "2018-10-15 13:28:19,646 INFO Configuration: {'data_config': {'data_path': '$SCRATCH/pytorch-mnist/data', 'name': 'mnist'}, 'experiment_config': {'name': 'basic', 'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/output'}, 'model_config': {'conv_sizes': [8, 16, 32], 'dense_sizes': [], 'input_shape': [1, 28, 28], 'learning_rate': 0.001, 'model_type': 'cnn_classifier', 'n_classes': 10, 'optimizer': 'Adam'}, 'train_config': {'batch_size': 64, 'n_epochs': 1}}\n",
      "2018-10-15 13:28:20,225 INFO Loaded 60000 training samples\n",
      "2018-10-15 13:28:20,226 INFO Loaded 10000 validation samples\n",
      "2018-10-15 13:28:20,274 INFO Model: \n",
      "CNNClassifier(\n",
      "  (conv_net): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense_net): Sequential(\n",
      "    (0): Linear(in_features=288, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameters: 8778\n",
      "2018-10-15 13:28:20,275 INFO Epoch 0\n",
      "2018-10-15 13:28:45,576 INFO   Training loss: 0.262\n",
      "2018-10-15 13:28:48,011 INFO   Validation loss: 0.084 acc: 0.973\n",
      "2018-10-15 13:28:48,016 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/output/summaries.npz\n",
      "2018-10-15 13:28:48,030 INFO Finished training\n",
      "2018-10-15 13:28:48,030 INFO Train samples 60000 time 25.3012s rate 2371.43 samples/s\n",
      "2018-10-15 13:28:48,031 INFO Valid samples 10000 time 2.43356 s rate 4109.21 samples/s\n",
      "2018-10-15 13:28:48,031 INFO All done!\n"
     ]
    }
   ],
   "source": [
    "!python ./main.py $config_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trun training on batch system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job configuration\n",
    "job_config = dict(\n",
    "    node_type='haswell',\n",
    "    n_nodes=1,\n",
    "    qos='interactive',\n",
    "    time=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "salloc: Pending job allocation 15769472\n",
      "salloc: job 15769472 queued and waiting for resources\n",
      "salloc: job 15769472 has been allocated resources\n",
      "salloc: Granted job allocation 15769472\n",
      "salloc: Waiting for resource configuration\n",
      "salloc: Nodes nid00046 are ready for job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job = SlurmJob(**job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, err = job.submit_task('python ./main.py %s' % config_file).communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "2018-10-15 13:45:47,406 INFO Initializing\n",
      "2018-10-15 13:45:47,925 INFO Configuration: {'data_config': {'data_path': '$SCRATCH/pytorch-mnist/data', 'name': 'mnist'}, 'experiment_config': {'name': 'basic', 'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/output'}, 'model_config': {'conv_sizes': [8, 16, 32], 'dense_sizes': [], 'input_shape': [1, 28, 28], 'learning_rate': 0.001, 'model_type': 'cnn_classifier', 'n_classes': 10, 'optimizer': 'Adam'}, 'train_config': {'batch_size': 64, 'n_epochs': 1}}\n",
      "2018-10-15 13:45:48,601 INFO Loaded 60000 training samples\n",
      "2018-10-15 13:45:48,601 INFO Loaded 10000 validation samples\n",
      "2018-10-15 13:45:48,678 INFO Model: \n",
      "CNNClassifier(\n",
      "  (conv_net): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense_net): Sequential(\n",
      "    (0): Linear(in_features=288, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameters: 8778\n",
      "2018-10-15 13:45:48,678 INFO Epoch 0\n",
      "2018-10-15 13:46:07,494 INFO   Training loss: 0.275\n",
      "2018-10-15 13:46:09,356 INFO   Validation loss: 0.139 acc: 0.953\n",
      "2018-10-15 13:46:09,359 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/output/summaries.npz\n",
      "2018-10-15 13:46:09,373 INFO Finished training\n",
      "2018-10-15 13:46:09,374 INFO Train samples 60000 time 18.8157s rate 3188.82 samples/s\n",
      "2018-10-15 13:46:09,374 INFO Valid samples 10000 time 1.86111 s rate 5373.15 samples/s\n",
      "2018-10-15 13:46:09,374 INFO All done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our python logging currently goes to stderr\n",
    "print(err.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the allocation\n",
    "del job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation\n",
    "\n",
    "To evaluate a model, we retrieve its output which was saved to the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_acc(config):\n",
    "    output_dir = os.path.expandvars(config['experiment_config']['output_dir'])\n",
    "    summaries = np.load(os.path.join(output_dir, 'summaries.npz'))\n",
    "    return summaries['valid_acc'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_val_acc(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define HP sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hpo_trials = 2\n",
    "\n",
    "# Hyper-parameters for model config\n",
    "c1 = np.random.choice([4, 8, 16], size=n_hpo_trials)\n",
    "c2 = np.random.choice([4, 8, 16], size=n_hpo_trials)\n",
    "c3 = np.random.choice([8, 16, 32], size=n_hpo_trials)\n",
    "lr = np.random.choice([0.0001, 0.001, 0.01], size=n_hpo_trials)\n",
    "conv_sizes = np.stack([c1, c2, c3], axis=1)\n",
    "\n",
    "# Training config\n",
    "batch_size = 64\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the configurations for the HPO tasks\n",
    "configs = [\n",
    "    build_config(output_dir=os.path.join(hpo_dir, 'hp_%i' % i),\n",
    "                 conv_sizes=conv_sizes[i], dense_sizes=[], learning_rate=lr[i],\n",
    "                 batch_size=batch_size, n_epochs=n_epochs)\n",
    "    for i in range(n_hpo_trials)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch HP tasks to batch system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job configuration\n",
    "job_config = dict(\n",
    "    node_type='haswell',\n",
    "    n_nodes=1,\n",
    "    qos='interactive',\n",
    "    time=30,\n",
    ")\n",
    "\n",
    "# Fix thread settings for remote job\n",
    "#os.environ['OMP_NUM_THREADS'] = '32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "salloc: Pending job allocation 15770421\n",
      "salloc: job 15770421 queued and waiting for resources\n",
      "salloc: job 15770421 has been allocated resources\n",
      "salloc: Granted job allocation 15770421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the job\n",
    "job = SlurmJob(**job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, config in enumerate(configs):\n",
    "    output_dir = config['experiment_config']['output_dir']\n",
    "    \n",
    "    # Write the configuration to file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    config_file = os.path.join(output_dir, 'config.yaml')\n",
    "    with open(config_file, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    # Submit the task\n",
    "    results.append(job.submit_task('python ./main.py %s' % config_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15770421     allocation interacti+    dasrepo         64    RUNNING      0:0 \n",
      "15770421.ex+     extern               dasrepo         64    RUNNING      0:0 \n",
      "15770421.0       python               dasrepo          1  COMPLETED      0:0 \n",
      "15770421.1       python               dasrepo          1  COMPLETED      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct | tail -n 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = results[0].communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "2018-10-15 15:13:39,559 INFO Initializing\n",
      "2018-10-15 15:13:39,626 INFO Configuration: {'data_config': {'data_path': '$SCRATCH/pytorch-mnist/data', 'name': 'mnist'}, 'experiment_config': {'name': 'basic', 'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_0'}, 'model_config': {'conv_sizes': array([4, 8, 8]), 'dense_sizes': [], 'input_shape': [1, 28, 28], 'learning_rate': 0.0001, 'model_type': 'cnn_classifier', 'n_classes': 10, 'optimizer': 'Adam'}, 'train_config': {'batch_size': 64, 'n_epochs': 2}}\n",
      "2018-10-15 15:13:40,228 INFO Loaded 60000 training samples\n",
      "2018-10-15 15:13:40,228 INFO Loaded 10000 validation samples\n",
      "2018-10-15 15:13:40,324 INFO Model: \n",
      "CNNClassifier(\n",
      "  (conv_net): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense_net): Sequential(\n",
      "    (0): Linear(in_features=72, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameters: 1650\n",
      "2018-10-15 15:13:40,324 INFO Epoch 0\n",
      "2018-10-15 15:13:54,055 INFO   Training loss: 1.568\n",
      "2018-10-15 15:13:55,530 INFO   Validation loss: 0.651 acc: 0.815\n",
      "2018-10-15 15:13:55,533 INFO Epoch 1\n",
      "2018-10-15 15:14:09,240 INFO   Training loss: 0.508\n",
      "2018-10-15 15:14:10,716 INFO   Validation loss: 0.390 acc: 0.887\n",
      "2018-10-15 15:14:10,719 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_0/summaries.npz\n",
      "2018-10-15 15:14:10,736 INFO Finished training\n",
      "2018-10-15 15:14:10,736 INFO Train samples 60000 time 13.7183s rate 4373.73 samples/s\n",
      "2018-10-15 15:14:10,736 INFO Valid samples 10000 time 1.47566 s rate 6776.63 samples/s\n",
      "2018-10-15 15:14:10,736 INFO All done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stderr.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8872"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_val_acc(configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9315"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_val_acc(configs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!module load /usr/common/software/modulefiles/pytorch-mpi/v0.4.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-v0.4.1",
   "language": "python",
   "name": "pytorch-v0.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
