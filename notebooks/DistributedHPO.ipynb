{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Hyper-parameter Optimization on Cori notebook\n",
    "\n",
    "In this notebook we will develop a Cori example for HPO.\n",
    "\n",
    "Let's develop the notebook in stages:\n",
    "1. Make it so we can invoke python and run training of a single job from the notebook on the login node.\n",
    "2. Launch the single job on the batch system using the SlurmJob API.\n",
    "3. Develop HPO logic\n",
    "4. Demonstrate distributed HPO with multi-node training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/s/sfarrell/WorkAreas/jupyter-dl/pytorch-examples\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.slurm_helpers import SlurmJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "Let's start by editing our environment directly.\n",
    "I might also prefer to do this in the kernel json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] = '/usr/common/software/pytorch/v0.4.1/bin:' + os.environ['PATH']\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/common/software/pytorch/v0.4.1/lib:' + os.environ['LD_LIBRARY_PATH']\n",
    "os.environ['MPICH_MAX_THREAD_SAFETY'] = 'multiple'\n",
    "os.environ['KMP_AFFINITY'] = 'granularity=fine,compact,1,0'\n",
    "os.environ['KMP_BLOCKTIME'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_dir = os.path.expandvars('$SCRATCH/pytorch-examples/mnist-hpo')\n",
    "os.makedirs(hpo_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(output_dir, conv_sizes, dense_sizes,\n",
    "                 learning_rate=0.001, optimizer='Adam',\n",
    "                 batch_size=64, n_epochs=1):\n",
    "    data_config = dict(name='mnist', data_path='$SCRATCH/pytorch-mnist/data')\n",
    "    experiment_config = dict(name='basic', output_dir=output_dir)\n",
    "    model_config = dict(\n",
    "        model_type='cnn_classifier',\n",
    "        input_shape=[1, 28, 28], n_classes=10,\n",
    "        conv_sizes=conv_sizes, dense_sizes=dense_sizes,\n",
    "        optimizer=optimizer, learning_rate=learning_rate\n",
    "    )\n",
    "    train_config = dict(batch_size=batch_size, n_epochs=n_epochs)\n",
    "    return dict(data_config=data_config, experiment_config=experiment_config,\n",
    "                model_config=model_config, train_config=train_config)\n",
    "\n",
    "def get_val_acc(config):\n",
    "    output_dir = os.path.expandvars(config['experiment_config']['output_dir'])\n",
    "    summaries = np.load(os.path.join(output_dir, 'summaries.npz'))\n",
    "    return summaries['valid_acc'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = build_config(output_dir=os.path.join(hpo_dir, 'output'),\n",
    "                      conv_sizes=[8, 16, 32], dense_sizes=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the configuration to a temporary file\n",
    "config_file = os.path.join(hpo_dir, 'test.yaml')\n",
    "with open(config_file, 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./main.py $config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation set accuracy:', get_val_acc(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training on batch system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job configuration\n",
    "job_config = dict(\n",
    "    node_type='haswell',\n",
    "    n_nodes=1,\n",
    "    qos='interactive',\n",
    "    time=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = SlurmJob(**job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, err = job.submit_task('python ./main.py %s' % config_file).communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our python logging currently goes to stderr\n",
    "print(err.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation set accuracy:', get_val_acc(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the allocation\n",
    "del job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define HP sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hpo_trials = 8\n",
    "\n",
    "# Hyper-parameters for model config\n",
    "c1 = np.random.choice([4, 8, 16], size=n_hpo_trials)\n",
    "c2 = np.random.choice([4, 8, 16], size=n_hpo_trials)\n",
    "c3 = np.random.choice([8, 16, 32], size=n_hpo_trials)\n",
    "lr = np.random.choice([0.0001, 0.001, 0.01], size=n_hpo_trials)\n",
    "conv_sizes = np.stack([c1, c2, c3], axis=1)\n",
    "\n",
    "# Training config\n",
    "batch_size = 64\n",
    "n_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the configurations for the HPO tasks\n",
    "configs = [\n",
    "    build_config(output_dir=os.path.join(hpo_dir, 'hp_%i' % i),\n",
    "                 conv_sizes=conv_sizes[i], dense_sizes=[], learning_rate=lr[i],\n",
    "                 batch_size=batch_size, n_epochs=n_epochs)\n",
    "    for i in range(n_hpo_trials)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch HP tasks to batch system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job configuration\n",
    "job_config = dict(\n",
    "    node_type='haswell',\n",
    "    n_nodes=16,\n",
    "    qos='interactive',\n",
    "    time='2:00:00',\n",
    ")\n",
    "\n",
    "# Fix thread settings for remote job\n",
    "os.environ['OMP_NUM_THREADS'] = '32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "salloc: Pending job allocation 15798824\n",
      "salloc: job 15798824 queued and waiting for resources\n",
      "salloc: job 15798824 has been allocated resources\n",
      "salloc: Granted job allocation 15798824\n",
      "salloc: Waiting for resource configuration\n",
      "salloc: Nodes nid000[83-98] are ready for job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the job\n",
    "job = SlurmJob(**job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-node training configuration\n",
    "n_nodes_per_task = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, config in enumerate(configs):\n",
    "    output_dir = config['experiment_config']['output_dir']\n",
    "    \n",
    "    # Write the configuration to file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    config_file = os.path.join(output_dir, 'config.yaml')\n",
    "    with open(config_file, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    # Submit the task\n",
    "    results.append(job.submit_task('python ./main.py -d %s' % config_file, n_nodes=n_nodes_per_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid = job.jobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode \n",
      "------------ ---------- ---------- ---------- ---------- ---------- -------- \n",
      "15798824     allocation interacti+    dasrepo       1024    RUNNING      0:0 \n",
      "15798824.ex+     extern               dasrepo       1024    RUNNING      0:0 \n",
      "15798824.0       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.1       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.2       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.3       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.4       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.5       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.6       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.7       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.8       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.9       python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.10      python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.11      python               dasrepo          4  COMPLETED      0:0 \n",
      "15798824.12      python               dasrepo          4    RUNNING      0:0 \n",
      "15798824.13      python               dasrepo          4    RUNNING      0:0 \n",
      "15798824.14      python               dasrepo          4    RUNNING      0:0 \n",
      "15798824.15      python               dasrepo          4    RUNNING      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j $jobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait and gather all the results\n",
    "outputs = [r.communicate() for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launched in background. Redirecting stdin to /dev/null\n",
      "srun: Job 15798824 step creation temporarily disabled, retrying\n",
      "srun: Step created for job 15798824\n",
      "2018-10-16 14:57:57,706 INFO Initializing\n",
      "2018-10-16 14:57:57,714 INFO Initializing\n",
      "2018-10-16 14:57:57,737 INFO Initializing\n",
      "2018-10-16 14:58:02,653 INFO Initializing\n",
      "2018-10-16 14:58:02,699 INFO MPI rank 0\n",
      "2018-10-16 14:58:02,701 INFO MPI rank 2\n",
      "2018-10-16 14:58:02,701 INFO MPI rank 3\n",
      "2018-10-16 14:58:02,700 INFO MPI rank 1\n",
      "2018-10-16 14:58:02,717 INFO Configuration: {'data_config': {'data_path': '$SCRATCH/pytorch-mnist/data', 'name': 'mnist'}, 'experiment_config': {'name': 'basic', 'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_0'}, 'model_config': {'conv_sizes': array([ 8, 16,  8]), 'dense_sizes': [], 'input_shape': [1, 28, 28], 'learning_rate': 0.001, 'model_type': 'cnn_classifier', 'n_classes': 10, 'optimizer': 'Adam'}, 'train_config': {'batch_size': 64, 'n_epochs': 4}}\n",
      "2018-10-16 14:58:03,192 INFO Loaded 60000 training samples\n",
      "2018-10-16 14:58:03,192 INFO Loaded 10000 validation samples\n",
      "2018-10-16 14:58:03,212 INFO Loaded 60000 training samples\n",
      "2018-10-16 14:58:03,212 INFO Loaded 10000 validation samples\n",
      "2018-10-16 14:58:03,225 INFO Epoch 0\n",
      "2018-10-16 14:58:03,224 INFO Model: \n",
      "DistributedDataParallelCPU(\n",
      "  (module): CNNClassifier(\n",
      "    (conv_net): Sequential(\n",
      "      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): ReLU()\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "      (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (dense_net): Sequential(\n",
      "      (0): Linear(in_features=72, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Parameters: 3138\n",
      "2018-10-16 14:58:03,224 INFO Epoch 0\n",
      "2018-10-16 14:58:03,239 INFO Loaded 60000 training samples\n",
      "2018-10-16 14:58:03,239 INFO Loaded 10000 validation samples\n",
      "2018-10-16 14:58:03,251 INFO Epoch 0\n",
      "2018-10-16 14:58:05,098 INFO Loaded 60000 training samples\n",
      "2018-10-16 14:58:05,098 INFO Loaded 10000 validation samples\n",
      "2018-10-16 14:58:05,110 INFO Epoch 0\n",
      "2018-10-16 14:58:09,881 INFO   Training loss: 0.745\n",
      "2018-10-16 14:58:09,883 INFO   Training loss: 0.751\n",
      "2018-10-16 14:58:09,881 INFO   Training loss: 0.739\n",
      "2018-10-16 14:58:09,882 INFO   Training loss: 0.748\n",
      "2018-10-16 14:58:11,659 INFO   Validation loss: 0.215 acc: 0.936\n",
      "2018-10-16 14:58:11,659 INFO Epoch 1\n",
      "2018-10-16 14:58:11,669 INFO   Validation loss: 0.215 acc: 0.936\n",
      "2018-10-16 14:58:11,669 INFO Epoch 1\n",
      "2018-10-16 14:58:11,673 INFO   Validation loss: 0.215 acc: 0.936\n",
      "2018-10-16 14:58:11,673 INFO Epoch 1\n",
      "2018-10-16 14:58:11,679 INFO   Validation loss: 0.215 acc: 0.936\n",
      "2018-10-16 14:58:11,682 INFO Epoch 1\n",
      "2018-10-16 14:58:16,329 INFO   Training loss: 0.176\n",
      "2018-10-16 14:58:16,328 INFO   Training loss: 0.180\n",
      "2018-10-16 14:58:16,329 INFO   Training loss: 0.180\n",
      "2018-10-16 14:58:16,328 INFO   Training loss: 0.182\n",
      "2018-10-16 14:58:18,105 INFO   Validation loss: 0.136 acc: 0.959\n",
      "2018-10-16 14:58:18,105 INFO Epoch 2\n",
      "2018-10-16 14:58:18,113 INFO   Validation loss: 0.136 acc: 0.959\n",
      "2018-10-16 14:58:18,113 INFO Epoch 2\n",
      "2018-10-16 14:58:18,115 INFO   Validation loss: 0.136 acc: 0.959\n",
      "2018-10-16 14:58:18,115 INFO Epoch 2\n",
      "2018-10-16 14:58:18,130 INFO   Validation loss: 0.136 acc: 0.959\n",
      "2018-10-16 14:58:18,133 INFO Epoch 2\n",
      "2018-10-16 14:58:22,738 INFO   Training loss: 0.132\n",
      "2018-10-16 14:58:22,739 INFO   Training loss: 0.131\n",
      "2018-10-16 14:58:22,738 INFO   Training loss: 0.135\n",
      "2018-10-16 14:58:22,739 INFO   Training loss: 0.129\n",
      "2018-10-16 14:58:24,515 INFO   Validation loss: 0.104 acc: 0.970\n",
      "2018-10-16 14:58:24,515 INFO Epoch 3\n",
      "2018-10-16 14:58:24,522 INFO   Validation loss: 0.104 acc: 0.970\n",
      "2018-10-16 14:58:24,522 INFO Epoch 3\n",
      "2018-10-16 14:58:24,523 INFO   Validation loss: 0.104 acc: 0.970\n",
      "2018-10-16 14:58:24,523 INFO Epoch 3\n",
      "2018-10-16 14:58:24,537 INFO   Validation loss: 0.104 acc: 0.970\n",
      "2018-10-16 14:58:24,540 INFO Epoch 3\n",
      "2018-10-16 14:58:29,165 INFO   Training loss: 0.108\n",
      "2018-10-16 14:58:29,163 INFO   Training loss: 0.112\n",
      "2018-10-16 14:58:29,164 INFO   Training loss: 0.105\n",
      "2018-10-16 14:58:29,163 INFO   Training loss: 0.109\n",
      "2018-10-16 14:58:30,949 INFO   Validation loss: 0.088 acc: 0.974\n",
      "2018-10-16 14:58:30,949 INFO Finished training\n",
      "2018-10-16 14:58:30,949 INFO Train samples 15000 time 5.15272s rate 2911.08 samples/s\n",
      "2018-10-16 14:58:30,949 INFO Valid samples 10000 time 1.77756 s rate 5625.69 samples/s\n",
      "2018-10-16 14:58:30,949 INFO All done!\n",
      "2018-10-16 14:58:30,955 INFO   Validation loss: 0.088 acc: 0.974\n",
      "2018-10-16 14:58:30,955 INFO Finished training\n",
      "2018-10-16 14:58:30,956 INFO Train samples 15000 time 5.13932s rate 2918.68 samples/s\n",
      "2018-10-16 14:58:30,956 INFO Valid samples 10000 time 1.78625 s rate 5598.31 samples/s\n",
      "2018-10-16 14:58:30,956 INFO All done!\n",
      "2018-10-16 14:58:30,957 INFO   Validation loss: 0.088 acc: 0.974\n",
      "2018-10-16 14:58:30,957 INFO Finished training\n",
      "2018-10-16 14:58:30,957 INFO Train samples 15000 time 4.67255s rate 3210.24 samples/s\n",
      "2018-10-16 14:58:30,957 INFO Valid samples 10000 time 1.78865 s rate 5590.81 samples/s\n",
      "2018-10-16 14:58:30,958 INFO All done!\n",
      "2018-10-16 14:58:30,975 INFO   Validation loss: 0.088 acc: 0.974\n",
      "2018-10-16 14:58:30,978 INFO Saving summaries to /global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_0/summaries.npz\n",
      "2018-10-16 14:58:30,989 INFO Finished training\n",
      "2018-10-16 14:58:30,989 INFO Train samples 15000 time 5.13245s rate 2922.58 samples/s\n",
      "2018-10-16 14:58:30,989 INFO Valid samples 10000 time 1.80232 s rate 5548.39 samples/s\n",
      "2018-10-16 14:58:30,989 INFO All done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the full output from one job\n",
    "print(outputs[0][1].decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracies: [0.9742 0.976  0.9841 0.9747 0.9778 0.8979 0.9776 0.8976]\n"
     ]
    }
   ],
   "source": [
    "# Gather the validation set accuracies\n",
    "val_accs = np.array([get_val_acc(config) for config in configs])\n",
    "print('Validation set accuracies:', val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_config': {'name': 'mnist', 'data_path': '$SCRATCH/pytorch-mnist/data'},\n",
       " 'experiment_config': {'name': 'basic',\n",
       "  'output_dir': '/global/cscratch1/sd/sfarrell/pytorch-examples/mnist-hpo/hp_2'},\n",
       " 'model_config': {'model_type': 'cnn_classifier',\n",
       "  'input_shape': [1, 28, 28],\n",
       "  'n_classes': 10,\n",
       "  'conv_sizes': array([ 8,  4, 32]),\n",
       "  'dense_sizes': [],\n",
       "  'optimizer': 'Adam',\n",
       "  'learning_rate': 0.01},\n",
       " 'train_config': {'batch_size': 64, 'n_epochs': 4}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model configuration\n",
    "configs[val_accs.argmax()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-v0.4.1",
   "language": "python",
   "name": "pytorch-v0.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
